{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kyuho\\anaconda3\\envs\\helix\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "# LEARNING_RATE = 0.001\n",
    "EPOCH = 50\n",
    "\n",
    "RESULTS_PATH = 'autoencoder_points\\\\results2\\\\'\n",
    "DATASET_PATH = 'tracks_1m_updated_asymmetric.txt'\n",
    "ENCODER_PATH = 'asymmetric\\\\results2_trig_minmax\\\\encoder_epoch_50.pth'\n",
    "DECODER_PATH = 'asymmetric\\\\results2_trig_minmax\\\\decoder_epoch_50.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        with open(path, 'r') as file:\n",
    "            content = file.read()\n",
    "            data_points = content.split('EOT')\n",
    "\n",
    "            data_points = [dp.strip() for dp in data_points if dp.strip()]\n",
    "            data_points = [dp.split('\\n') for dp in data_points]\n",
    "            data_points = [[[float(cell) for cell in row.split(', ')] for row in dp] for dp in data_points]\n",
    "            self.original_targets = np.array([dp[0] for dp in data_points])\n",
    "            input_points = [dp[1:] for dp in data_points]\n",
    "            targets_2 = np.delete(self.original_targets, 1, 1)\n",
    "            targets_2 = np.hstack((targets_2, np.cos(self.original_targets[:, 1])[..., None]))\n",
    "            targets_cos_sin = np.hstack((targets_2, np.sin(self.original_targets[:, 1])[..., None]))\n",
    "            self.scaler = MinMaxScaler()\n",
    "            self.rescaled_targets = self.scaler.fit_transform(targets_cos_sin)\n",
    "            self.rescaled_targets = torch.tensor(self.rescaled_targets)\n",
    "            self.original_targets = torch.tensor(targets_cos_sin)\n",
    "            inputs = []\n",
    "            for input in input_points:\n",
    "                combined = []\n",
    "                for coordinate in input:\n",
    "                    combined += coordinate\n",
    "                inputs.append(combined)\n",
    "            self.inputs = torch.tensor(np.array(inputs))\n",
    "            # self.inputs = torch.tensor(np.array(input_points))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rescaled_targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target = self.rescaled_targets[idx]\n",
    "        input = self.inputs[idx]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.layer1 = nn.Linear(30, 32)\n",
    "        # self.layer2 = nn.Linear(32, 64)\n",
    "        # self.layer3 = nn.Linear(64, 5)\n",
    "        self.layer1 = nn.Linear(30, 200)\n",
    "        self.layer2 = nn.Linear(200, 400)\n",
    "        self.layer3 = nn.Linear(400, 800)\n",
    "        self.layer4 = nn.Linear(800, 800)\n",
    "        self.layer5 = nn.Linear(800, 800)\n",
    "        self.layer6 = nn.Linear(800, 400)\n",
    "        self.layer7 = nn.Linear(400, 200)\n",
    "        self.output_layer = nn.Linear(200, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.layer1(x))\n",
    "        x = F.leaky_relu(self.layer2(x))\n",
    "        x = F.leaky_relu(self.layer3(x))\n",
    "        x = F.leaky_relu(self.layer4(x))\n",
    "        x = F.leaky_relu(self.layer5(x))\n",
    "        x = F.leaky_relu(self.layer6(x))\n",
    "        x = F.leaky_relu(self.layer7(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.layer1 = nn.Linear(5, 64)\n",
    "        # self.layer2 = nn.Linear(64, 32)\n",
    "        # self.layer3 = nn.Linear(32, 30)\n",
    "        self.layer1 = nn.Linear(6, 200)\n",
    "        self.layer2 = nn.Linear(200, 200)\n",
    "        self.layer3 = nn.Linear(200, 200)\n",
    "        self.layer4 = nn.Linear(200, 400)\n",
    "        self.layer5 = nn.Linear(400, 800)\n",
    "        self.layer6 = nn.Linear(800, 800)\n",
    "        self.layer7 = nn.Linear(800, 800)\n",
    "        self.layer8 = nn.Linear(800, 400)\n",
    "        self.layer9 = nn.Linear(400, 200)\n",
    "        self.output_layer = nn.Linear(200, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = F.relu(self.layer5(x))\n",
    "        x = F.relu(self.layer6(x))\n",
    "        x = F.relu(self.layer7(x))\n",
    "        x = F.relu(self.layer8(x))\n",
    "        x = F.relu(self.layer9(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_distance(encoder, decoder, encoder_optimizer, decoder_optimizer, encoder_scheduler, decoder_scheduler, val_dl, device, prev_encoder_path, prev_decoder_path, data_size, threshold=1):\n",
    "    print(f\"Loading model from {prev_encoder_path}\")\n",
    "    checkpoint = torch.load(prev_encoder_path)\n",
    "    encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    print(f\"Loading model from {prev_decoder_path}\")\n",
    "    checkpoint = torch.load(prev_decoder_path)\n",
    "    decoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = torch.zeros(data_size)\n",
    "        targets = torch.zeros(data_size)\n",
    "        distances = torch.zeros(data_size)\n",
    "        current_size = 0\n",
    "        for input, target in val_dl:\n",
    "            input = input.float().to(device)\n",
    "            target = target.float().to(device)\n",
    "            output_points = decoder(encoder(input))\n",
    "            first_dim = torch.numel(output_points)  // 30\n",
    "            output_points = output_points.reshape(first_dim, 10, 3)\n",
    "            reshaped_points = input.reshape(first_dim, 10, 3)\n",
    "            distance = torch.norm(output_points - reshaped_points, 2, dim=(1, 2))\n",
    "            # distance = torch.sum(torch.square(output_points - reshaped_points), dim=(1, 2))\n",
    "            distances[current_size:current_size + distance.shape[0]] = distance\n",
    "            output = (distance < threshold).float()\n",
    "            predictions[current_size:current_size + output.shape[0]] = output\n",
    "            targets[current_size:current_size + target.shape[0]] = target\n",
    "            current_size += distance.shape[0]\n",
    "\n",
    "    print(distances[-1])\n",
    "    print(distances)\n",
    "    print(distances.shape)\n",
    "    print(torch.mean(distances))\n",
    "    print(\"predictions:\", predictions)\n",
    "    print(\"targets:\", targets)\n",
    "    print(\"predicted helix but was actually non-helix\")\n",
    "    predict_helix = predictions == 1\n",
    "    wrong = predictions != targets\n",
    "    mask = torch.logical_and(predict_helix, wrong)\n",
    "    print(predictions[mask].size())\n",
    "    print(\"predicted non-helix but was helix\")\n",
    "    predict_non_helix = predictions == 0\n",
    "    wrong = predictions != targets\n",
    "    mask = torch.logical_and(predict_non_helix, wrong)\n",
    "    print(predictions[mask].size())\n",
    "    print((predictions == targets).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    dataset = Dataset(DATASET_PATH)\n",
    "    train_size = int(TRAIN_RATIO * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    encoder = Encoder()\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = Decoder()\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()\n",
    "    encoder_criterion = nn.MSELoss()\n",
    "    decoder_criterion = nn.MSELoss()\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr = LEARNING_RATE)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr = LEARNING_RATE)\n",
    "    encoder_scheduler = torch.optim.lr_scheduler.MultiStepLR(encoder_optimizer, milestones=[15, 30, 50], gamma=0.5)\n",
    "    decoder_scheduler = torch.optim.lr_scheduler.MultiStepLR(decoder_optimizer, milestones=[15, 30, 50], gamma=0.1)\n",
    "    \n",
    "    test_distance(encoder, decoder, encoder_optimizer, decoder_optimizer, encoder_scheduler, decoder_scheduler, val_dl=dataloader, device=device, \n",
    "                  prev_encoder_path=ENCODER_PATH, prev_decoder_path=DECODER_PATH, data_size=len(dataset), threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from asymmetric\\results2_trig_minmax\\encoder_epoch_50.pth\n",
      "Loading model from asymmetric\\results2_trig_minmax\\decoder_epoch_50.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyuho\\AppData\\Local\\Temp\\ipykernel_13296\\3889688334.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_encoder_path)\n",
      "C:\\Users\\Kyuho\\AppData\\Local\\Temp\\ipykernel_13296\\3889688334.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_decoder_path)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 39740, 33516, 39780, 23828) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Kyuho\\anaconda3\\envs\\helix\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Kyuho\\anaconda3\\envs\\helix\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m encoder_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mMultiStepLR(encoder_optimizer, milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m50\u001b[39m], gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     24\u001b[0m decoder_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mMultiStepLR(decoder_optimizer, milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m50\u001b[39m], gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtest_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m              \u001b[49m\u001b[43mprev_encoder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENCODER_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_decoder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDECODER_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mtest_distance\u001b[1;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, encoder_scheduler, decoder_scheduler, val_dl, device, prev_encoder_path, prev_decoder_path, data_size, threshold)\u001b[0m\n\u001b[0;32m     15\u001b[0m distances \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(data_size)\n\u001b[0;32m     16\u001b[0m current_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kyuho\\anaconda3\\envs\\helix\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Kyuho\\anaconda3\\envs\\helix\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kyuho\\anaconda3\\envs\\helix\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Kyuho\\anaconda3\\envs\\helix\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1144\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1143\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 39740, 33516, 39780, 23828) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
