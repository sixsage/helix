{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "\n",
    "TRAIN_PATH = 'C:\\\\Users\\\\sixsa\\\\code\\\\UCI\\\\helix\\\\tracks_1m_updated_asymmetric_higher.txt'\n",
    "HELIX_VAL_PATH = 'C:\\\\Users\\\\sixsa\\\\code\\\\UCI\\\\helix\\\\tracks_100k_updated_asymmetric_higher.txt'\n",
    "NON_HELIX_VAL_PATH = 'C:\\\\Users\\\\sixsa\\\\code\\\\UCI\\\\helix\\\\sintracks_100k_updated_asymmetric_higher.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 30)\n",
      "(100000, 5)\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_PATH, 'r') as file:\n",
    "    content = file.read()\n",
    "    data_points = content.split('EOT')\n",
    "\n",
    "    data_points = [dp.strip() for dp in data_points if dp.strip()]\n",
    "    data_points = [dp.split('\\n') for dp in data_points]\n",
    "    data_points = [[[float(cell) for cell in row.split(', ')] for row in dp] for dp in data_points]\n",
    "    targets = [dp[0] for dp in data_points]\n",
    "    PARAMETERS_TRAINING = np.array(targets)\n",
    "    input_points = [dp[1:] for dp in data_points]\n",
    "    inputs = []\n",
    "    for input in input_points:\n",
    "        combined = []\n",
    "        for coordinate in input:\n",
    "            combined += coordinate\n",
    "        inputs.append(combined)\n",
    "    POINTS_TRAINING = np.array(inputs)\n",
    "\n",
    "with open(HELIX_VAL_PATH, 'r') as file:\n",
    "    content = file.read()\n",
    "    data_points = content.split('EOT')\n",
    "\n",
    "    data_points = [dp.strip() for dp in data_points if dp.strip()]\n",
    "    data_points = [dp.split('\\n') for dp in data_points]\n",
    "    data_points = [[[float(cell) for cell in row.split(', ')] for row in dp] for dp in data_points]\n",
    "    targets = [dp[0] for dp in data_points]\n",
    "    PARAMETERS_VALIDATION_HELIX = np.array(targets)\n",
    "    input_points = [dp[1:] for dp in data_points]\n",
    "    inputs = []\n",
    "    for input in input_points:\n",
    "        combined = []\n",
    "        for coordinate in input:\n",
    "            combined += coordinate\n",
    "        inputs.append(combined)\n",
    "    POINTS_VALIDATION_HELIX = np.array(inputs)\n",
    "\n",
    "with open(NON_HELIX_VAL_PATH, 'r') as file:\n",
    "    content = file.read()\n",
    "    data_points = content.split('EOT')\n",
    "\n",
    "    data_points = [dp.strip() for dp in data_points if dp.strip()]\n",
    "    data_points = [dp.split('\\n') for dp in data_points]\n",
    "    data_points = [[[float(cell) for cell in row.split(', ')] for row in dp] for dp in data_points]\n",
    "    targets = [dp[0] for dp in data_points]\n",
    "    PARAMETERS_VALIDATION_NON_HELIX = np.array(targets)\n",
    "    input_points = [dp[1:] for dp in data_points]\n",
    "    inputs = []\n",
    "    for input in input_points:\n",
    "        combined = []\n",
    "        for coordinate in input:\n",
    "            combined += coordinate\n",
    "        inputs.append(combined)\n",
    "    POINTS_VALIDATION_NON_HELIX = np.array(inputs)\n",
    "print(POINTS_VALIDATION_HELIX.shape)\n",
    "print(PARAMETERS_VALIDATION_HELIX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    encoder = XGBRegressor(n_estimators=1500, device=\"cuda\")\n",
    "    encoder.fit(POINTS_TRAINING, PARAMETERS_TRAINING) # train \"encoder\"\n",
    "    # make encoder predict parameters\n",
    "    predicted_parameters = encoder.predict(POINTS_TRAINING)\n",
    "    decoder = XGBRegressor(n_estimators=1500, device=\"cuda\")\n",
    "    decoder.fit(predicted_parameters, POINTS_TRAINING)\n",
    "\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_parameter_error(encoder):\n",
    "    min_per_column = np.min(PARAMETERS_VALIDATION_HELIX, axis=0)\n",
    "    max_per_column = np.max(PARAMETERS_VALIDATION_HELIX, axis=0)\n",
    "    range_per_column = max_per_column - min_per_column\n",
    "    print(\"ranges\")\n",
    "    print(range_per_column)\n",
    "    predictions = encoder.predict(POINTS_VALIDATION_HELIX)\n",
    "    print(PARAMETERS_VALIDATION_HELIX)\n",
    "    print(predictions)\n",
    "    result_mae = np.abs(PARAMETERS_VALIDATION_HELIX - predictions)\n",
    "    result_mae = np.mean(result_mae, axis=0)\n",
    "    print('mae')\n",
    "    print(result_mae)\n",
    "    print('mae / range * 100')\n",
    "    percent_error = (result_mae / range_per_column) * 100\n",
    "    print(percent_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_classification(encoder, decoder, threshold):\n",
    "    predicted_points_helical = decoder.predict(encoder.predict(POINTS_VALIDATION_HELIX)) # this will be 2d\n",
    "    reshaped_val_helix_points = POINTS_VALIDATION_HELIX.reshape((POINTS_VALIDATION_HELIX.shape[0], 10, 3))\n",
    "    reshaped_predicted_points_helical = predicted_points_helical.reshape((POINTS_VALIDATION_HELIX.shape[0], 10, 3))\n",
    "    distances_helical = np.linalg.norm(reshaped_val_helix_points - reshaped_predicted_points_helical, axis=-1)\n",
    "    total_distance_per_entry_helical = np.sum(distances_helical, axis=-1)\n",
    "\n",
    "    predicted_points_helical = decoder.predict(encoder.predict(POINTS_VALIDATION_NON_HELIX)) # this will be 2d\n",
    "    reshaped_val_non_helix_points = POINTS_VALIDATION_HELIX.reshape((POINTS_VALIDATION_NON_HELIX.shape[0], 10, 3))\n",
    "    reshaped_predicted_points_non_helical = reshaped_val_non_helix_points.reshape((POINTS_VALIDATION_NON_HELIX.shape[0], 10, 3))\n",
    "    distances_helical = np.linalg.norm(reshaped_val_non_helix_points - reshaped_predicted_points_non_helical, axis=-1)\n",
    "    total_distance_per_entry_non_helical = np.sum(distances_helical, axis=-1)\n",
    "\n",
    "    combined_distances = np.concatenate((total_distance_per_entry_helical, total_distance_per_entry_non_helical), axis=0)\n",
    "\n",
    "    target = np.concatenate((np.ones(total_distance_per_entry_helical.shape), np.zeros(total_distance_per_entry_non_helical.shape)))\n",
    "    predictions = 1 * (combined_distances < threshold)\n",
    "    print(target)\n",
    "    print(predictions)\n",
    "\n",
    "    print(np.mean(target == predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sixsa\\anaconda3\\envs\\helix\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:35:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranges\n",
      "[4.50e-02 6.28e+00 1.75e+02 9.04e+00 2.67e+00]\n",
      "[[ 1.8900e-02  1.5200e+00  1.5884e+02  5.4000e-01 -1.0000e-01]\n",
      " [ 2.9000e-03  4.7000e+00  1.7385e+02  1.9300e+00 -5.3000e-01]\n",
      " [ 3.5000e-03  1.0000e+00  7.0020e+01  8.0000e-01  4.7000e-01]\n",
      " ...\n",
      " [ 4.9000e-03  3.7000e-01  1.6733e+02  6.9000e-01  2.2000e-01]\n",
      " [ 7.6000e-03  6.0900e+00  1.6003e+02 -8.0000e-01  4.7000e-01]\n",
      " [ 1.6100e-02  2.4900e+00  1.8783e+02  1.7900e+00  1.9000e-01]]\n",
      "[[ 7.8018652e-03  1.5185254e+00  1.6725746e+02  5.4300082e-01\n",
      "  -9.6691042e-02]\n",
      " [ 1.1893836e-02  4.6918778e+00  1.4604335e+02  1.9221940e+00\n",
      "  -5.3957123e-01]\n",
      " [ 1.1871860e-02  1.0030295e+00  6.8422546e+01  7.9901224e-01\n",
      "   4.7254717e-01]\n",
      " ...\n",
      " [ 9.0857008e-03  3.6029395e-01  1.7869379e+02  6.9477385e-01\n",
      "   2.1904367e-01]\n",
      " [ 7.4387845e-03  6.0914426e+00  1.6330972e+02 -7.8289264e-01\n",
      "   4.7350496e-01]\n",
      " [ 1.4929137e-02  2.5007203e+00  1.7228375e+02  1.7729076e+00\n",
      "   1.8790159e-01]]\n",
      "mae\n",
      "[4.35857459e-03 1.31228874e-02 6.03657407e+00 9.40111038e-03\n",
      " 3.37082843e-03]\n",
      "mae / range * 100\n",
      "[9.6857213  0.20896318 3.4494709  0.10399458 0.12624826]\n"
     ]
    }
   ],
   "source": [
    "calc_parameter_error(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[1 1 1 ... 1 1 1]\n",
      "0.49738\n"
     ]
    }
   ],
   "source": [
    "calc_classification(encoder, decoder, threshold=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
