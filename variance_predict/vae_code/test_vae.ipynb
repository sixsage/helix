{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kyuho\\anaconda3\\envs\\helix\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.0001\n",
    "# LEARNING_RATE = 0.001\n",
    "ENCODER_EPOCH = 100\n",
    "DECODER_EPOCH = 0\n",
    "REPORT_PATH = 'experiments.txt'\n",
    "\n",
    "LAYER_ARR = [200, 400, 800, 1600, 1600, 1600, 800, 400, 400, 400, 200, 100]\n",
    "KLD_WEIGHT = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixture\n",
    "# DATASET_PATH = 'non_gaussian_tracks\\\\tracks_1M_gaussian_mixturev2.txt'\n",
    "# VAL_DATASET_PATH = 'non_gaussian_tracks\\\\tracks_100k_gaussian_mixturev2.txt'\n",
    "# ENCODER_RESULTS_PATH = 'variance_predict\\\\mixture\\\\results3\\\\'\n",
    "# DECODER_RESULTS_PATH = 'variance_predict\\\\mixture\\\\results3\\\\'\n",
    "\n",
    "DATASET_PATH = 'C:\\\\Users\\\\Kyuho\\\\code\\\\UCI\\\\helix\\\\tracks_100k_updated_test.txt'\n",
    "ENCODER_RESULTS_PATH = 'variance_predict\\\\gaussian\\\\vae_results1\\\\'\n",
    "\n",
    "# DATASET_PATH = 'tracks_1m_updated_asymmetric_higher.txt'\n",
    "# VAL_DATASET_PATH = 'tracks_100k_updated_asymmetric_higher.txt'\n",
    "# ENCODER_RESULTS_PATH = 'separated\\\\asymmetric\\\\results1\\\\'\n",
    "# DECODER_RESULTS_PATH = 'separated\\\\asymmetric\\\\results1\\\\'\n",
    "\n",
    "MODEL_PATH = 'C:\\\\Users\\\\Kyuho\\\\code\\\\UCI\\\\helix\\\\variance_predict\\\\gaussian\\\\vae_results1\\\\encoder_epoch_85.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        with open(path, 'r') as file:\n",
    "            content = file.read()\n",
    "            data_points = content.split('EOT')\n",
    "\n",
    "            data_points = [dp.strip() for dp in data_points if dp.strip()]\n",
    "            data_points = [dp.split('\\n') for dp in data_points]\n",
    "            data_points = [[[float(cell) for cell in row.split(', ')] for row in dp] for dp in data_points]\n",
    "            self.original_targets = np.array([dp[0] for dp in data_points])\n",
    "            input_points = [dp[1:] for dp in data_points]\n",
    "            self.scaler = MinMaxScaler()\n",
    "            self.original_targets_rescaled = self.scaler.fit_transform(self.original_targets)\n",
    "            self.original_targets_tensor = torch.tensor(self.original_targets)\n",
    "            inputs = []\n",
    "            for input in input_points:\n",
    "                combined = []\n",
    "                for coordinate in input:\n",
    "                    combined += coordinate\n",
    "                inputs.append(combined)\n",
    "            self.inputs = torch.tensor(np.array(inputs))\n",
    "            # self.inputs = torch.tensor(np.array(input_points))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_targets_rescaled)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target = self.original_targets_tensor[idx]\n",
    "        input = self.inputs[idx]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phi_inverse(target_radius_squared, d0, phi0, pt, dz, tanl, eps=1e-6):\n",
    "    alpha = 1 / 2  # 1/cB\n",
    "    q = 1\n",
    "    kappa = q / pt\n",
    "    rho = alpha / kappa\n",
    "\n",
    "    calculated_term_x = d0 * torch.cos(phi0) + rho * torch.cos(phi0) \n",
    "    calculated_term_y = d0 * torch.sin(phi0) + rho * torch.sin(phi0)\n",
    "\n",
    "    hypotenuse = torch.sqrt(calculated_term_x ** 2 + calculated_term_y ** 2)\n",
    "\n",
    "    arccos_input = (calculated_term_x ** 2 + calculated_term_y ** 2 + rho ** 2 - target_radius_squared) / (2 * rho * hypotenuse)\n",
    "    clamped = torch.clamp(arccos_input, min=-1 + eps, max=1-eps) # to avoid nan\n",
    "    arccos_term = torch.acos(clamped)\n",
    "    phi = arccos_term   \n",
    "\n",
    "    return phi % (2 * torch.pi) # wrap angle\n",
    "\n",
    "def compute_3d_track(phi, d0, phi0, pt, dz, tanl):\n",
    "    alpha = 1 / 2  # 1/cB\n",
    "    q = 1\n",
    "    kappa = q / pt\n",
    "    rho = alpha / kappa\n",
    "    x = d0 * torch.cos(phi0) + rho * (torch.cos(phi0) - torch.cos(phi0 + phi))\n",
    "    y = d0 * torch.sin(phi0) + rho * (torch.sin(phi0) - torch.sin(phi0 + phi))\n",
    "    z = dz - rho * tanl * phi\n",
    "    return x, y, z\n",
    "    \n",
    "# Generate noiseless hit points in 3D using optimized phi values\n",
    "def generate_noiseless_hits(params, min_radius=1.0, max_radius=10.0, num_layers=10):\n",
    "\n",
    "    d0, phi0, pt, dz, tanl = params[:, 0], params[:, 1], params[:, 2], params[:, 3], params[:, 4]\n",
    "    xs, ys, zs = [], [], []\n",
    "    target_radii = [torch.tensor(r, requires_grad=True) for r in torch.linspace(min_radius, max_radius, num_layers)]\n",
    "    #for target_radius in torch.linspace(min_radius, max_radius, num_layers, device=device):\n",
    "    for target_radius in target_radii:\n",
    "        # Optimize phi for the current radius\n",
    "        phi = find_phi_inverse(target_radius**2, d0, phi0, pt, dz, tanl)\n",
    "        x, y, z = compute_3d_track(phi, d0, phi0, pt, dz, tanl)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "\n",
    "    # Stack results into a tensor\n",
    "    hit_points = torch.stack((torch.stack(xs, dim=1), torch.stack(ys, dim=1), torch.stack(zs, dim=1)), dim=2)\n",
    "\n",
    "    #return hit_points, total_distance / num_layers, torch.stack(optimized_phi_values)\n",
    "    return hit_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateNoiselessHitsLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenerateNoiselessHitsLayer, self).__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        return generate_noiseless_hits(z)\n",
    "\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, input_size=30, hidden_sizes=LAYER_ARR, latent_size=5):\n",
    "        super().__init__()\n",
    "        layer_sizes = [input_size] + hidden_sizes\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(layer_sizes[i], layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)])\n",
    "        self.fc_mu = nn.Linear(hidden_sizes[-1], latent_size)\n",
    "        self.fc_logvar = nn.Linear(hidden_sizes[-1], latent_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # print(\"x input was \", x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        mu, logvar = self.fc_mu(x), self.fc_logvar(x)\n",
    "        # print(\"mu before was \", mu[0])\n",
    "        # print(\"logvar before was \", logvar[0])\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # print(\"z after reparameterize \", z[0])\n",
    "        return z, mu, logvar\n",
    "\n",
    "\n",
    "class VAE_EncoderWithHits(nn.Module):\n",
    "    def __init__(self, input_size=30, hidden_sizes=LAYER_ARR, latent_size=5):\n",
    "        super().__init__()\n",
    "        self.encoder = VAE_Encoder(input_size, hidden_sizes, latent_size)\n",
    "        self.noiseless_hits_layer = GenerateNoiselessHitsLayer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)   \n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        # print(\"z was \", z[0])\n",
    "        reconstructed_hits = self.noiseless_hits_layer(z)\n",
    "        # print(\"reconstructed_hits was \", reconstructed_hits[0])\n",
    "        return reconstructed_hits, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_differences_batch(set1, set2):\n",
    "    c2 = torch.sum((set1 - set2) ** 2, dim=(1, 2))\n",
    "    return torch.mean(c2)\n",
    "\n",
    "def compute_differences_no_mean(set1, set2):\n",
    "    return torch.sum((set1 - set2) ** 2, dim=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "# load model\n",
    "# run through the test data\n",
    "# compute 3d for both original parameter and the predicted parameter\n",
    "# get the distance\n",
    "def run_encoder(encoder, encoder_optimizer, dataloader, device, data_size, prev_encoder_path =''):\n",
    "    print(f\"Loading model from {prev_encoder_path}\")\n",
    "    checkpoint = torch.load(prev_encoder_path)\n",
    "    encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    distances = torch.zeros(data_size)\n",
    "    current_size = 0\n",
    "    for points, params in dataloader:\n",
    "        points = points.float().to(device)\n",
    "        params = params.float().to(device)\n",
    "        reconstructed_points, mean, logvar = encoder(points)\n",
    "        input_points = points.view(points.shape[0], 10, 3)\n",
    "        distance = compute_differences_no_mean(reconstructed_points, input_points)\n",
    "        distances[current_size:current_size + distance.shape[0]] = distance\n",
    "        current_size += distance.shape[0]\n",
    "        print(current_size)\n",
    "\n",
    "    return distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from C:\\Users\\Kyuho\\code\\UCI\\helix\\variance_predict\\gaussian\\vae_results1\\encoder_epoch_85.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyuho\\AppData\\Local\\Temp\\ipykernel_22304\\543877620.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(prev_encoder_path)\n",
      "C:\\Users\\Kyuho\\AppData\\Local\\Temp\\ipykernel_22304\\4172200342.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_radii = [torch.tensor(r, requires_grad=True) for r in torch.linspace(min_radius, max_radius, num_layers)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "1024\n",
      "1536\n",
      "2048\n",
      "2560\n",
      "3072\n",
      "3584\n",
      "4096\n",
      "4608\n",
      "5120\n",
      "5632\n",
      "6144\n",
      "6656\n",
      "7168\n",
      "7680\n",
      "8192\n",
      "8704\n",
      "9216\n",
      "9728\n",
      "10240\n",
      "10752\n",
      "11264\n",
      "11776\n",
      "12288\n",
      "12800\n",
      "13312\n",
      "13824\n",
      "14336\n",
      "14848\n",
      "15360\n",
      "15872\n",
      "16384\n",
      "16896\n",
      "17408\n",
      "17920\n",
      "18432\n",
      "18944\n",
      "19456\n",
      "19968\n",
      "20480\n",
      "20992\n",
      "21504\n",
      "22016\n",
      "22528\n",
      "23040\n",
      "23552\n",
      "24064\n",
      "24576\n",
      "25088\n",
      "25600\n",
      "26112\n",
      "26624\n",
      "27136\n",
      "27648\n",
      "28160\n",
      "28672\n",
      "29184\n",
      "29696\n",
      "30208\n",
      "30720\n",
      "31232\n",
      "31744\n",
      "32256\n",
      "32768\n",
      "33280\n",
      "33792\n",
      "34304\n",
      "34816\n",
      "35328\n",
      "35840\n",
      "36352\n",
      "36864\n",
      "37376\n",
      "37888\n",
      "38400\n",
      "38912\n",
      "39424\n",
      "39936\n",
      "40448\n",
      "40960\n",
      "41472\n",
      "41984\n",
      "42496\n",
      "43008\n",
      "43520\n",
      "44032\n",
      "44544\n",
      "45056\n",
      "45568\n",
      "46080\n",
      "46592\n",
      "47104\n",
      "47616\n",
      "48128\n",
      "48640\n",
      "49152\n",
      "49664\n",
      "50176\n",
      "50688\n",
      "51200\n",
      "51712\n",
      "52224\n",
      "52736\n",
      "53248\n",
      "53760\n",
      "54272\n",
      "54784\n",
      "55296\n",
      "55808\n",
      "56320\n",
      "56832\n",
      "57344\n",
      "57856\n",
      "58368\n",
      "58880\n",
      "59392\n",
      "59904\n",
      "60416\n",
      "60928\n",
      "61440\n",
      "61952\n",
      "62464\n",
      "62976\n",
      "63488\n",
      "64000\n",
      "64512\n",
      "65024\n",
      "65536\n",
      "66048\n",
      "66560\n",
      "67072\n",
      "67584\n",
      "68096\n",
      "68608\n",
      "69120\n",
      "69632\n",
      "70144\n",
      "70656\n",
      "71168\n",
      "71680\n",
      "72192\n",
      "72704\n",
      "73216\n",
      "73728\n",
      "74240\n",
      "74752\n",
      "75264\n",
      "75776\n",
      "76288\n",
      "76800\n",
      "77312\n",
      "77824\n",
      "78336\n",
      "78848\n",
      "79360\n",
      "79872\n",
      "80384\n",
      "80896\n",
      "81408\n",
      "81920\n",
      "82432\n",
      "82944\n",
      "83456\n",
      "83968\n",
      "84480\n",
      "84992\n",
      "85504\n",
      "86016\n",
      "86528\n",
      "87040\n",
      "87552\n",
      "88064\n",
      "88576\n",
      "89088\n",
      "89600\n",
      "90112\n",
      "90624\n",
      "91136\n",
      "91648\n",
      "92160\n",
      "92672\n",
      "93184\n",
      "93696\n",
      "94208\n",
      "94720\n",
      "95232\n",
      "95744\n",
      "96256\n",
      "96768\n",
      "97280\n",
      "97792\n",
      "98304\n",
      "98816\n",
      "99328\n",
      "99840\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "dataset = Dataset(DATASET_PATH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "encoder = VAE_EncoderWithHits()\n",
    "encoder = encoder.to(device)\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr = LEARNING_RATE)\n",
    "encoder_scheduler = None\n",
    "\n",
    "all_distances = run_encoder(encoder, encoder_optimizer, dataloader, device, len(dataset), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3419, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1393.9694, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0913, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances_list = all_distances.tolist()\n",
    "sorted_all_distances_list = sorted(all_distances_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.22881507873535"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_all_distances_list[-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(all_distances_list, bins=10, edgecolor='black')\n",
    "# plt.title('Data Distribution')\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_1 = 0\n",
    "less_4 = 0\n",
    "less_10 = 0\n",
    "less_100 = 0\n",
    "remaining = 0\n",
    "\n",
    "for distance in all_distances_list:\n",
    "    if distance < 1:\n",
    "        less_1 += 1\n",
    "    elif distance < 4:\n",
    "        less_4 += 1\n",
    "    elif distance < 10:\n",
    "        less_10 += 1\n",
    "    elif distance < 100:\n",
    "        less_100 += 1\n",
    "    else:\n",
    "        remaining += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36951 46728 14224 2095 2\n"
     ]
    }
   ],
   "source": [
    "print(less_1, less_4, less_10, less_100, remaining)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
